/Users/jason/anaconda3/envs/dain/bin/python /Users/jason/Documents/GitHub/DAIN_py/main.py
==> (Training video, Validation video):( 1320 489 )
39
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:48: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  init.kaiming_normal(self.feat.weight, mode='fan_out')
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:49: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(self.feat.bias, 0)
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:50: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(self.feat_bn.weight, 1)
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:51: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(self.feat_bn.bias, 0)
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:59: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(self.classifier.weight, std=0.001)
/Users/jason/Documents/GitHub/DAIN_py/models/resnet.py:60: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(self.classifier.bias, 0)
------train-----
3.6658642292022705 loss val
3.611083984375 loss val
3.511687994003296 loss val
3.616764783859253 loss val
3.4127931594848633 loss val
3.5162644386291504 loss val
3.386904001235962 loss val
3.242124319076538 loss val
3.410447597503662 loss val
3.124549627304077 loss val
2.9722304344177246 loss val
3.041893720626831 loss val
2.9938650131225586 loss val
2.786161422729492 loss val
2.7745397090911865 loss val
2.8950915336608887 loss val
2.491150379180908 loss val
2.232539415359497 loss val
2.212803602218628 loss val
2.5995044708251953 loss val
2.061095714569092 loss val
2.2656357288360596 loss val
1.9884979724884033 loss val
1.8319767713546753 loss val
1.829375147819519 loss val
1.9012302160263062 loss val
----validating-----
 * Prec@1 23.517 Prec@3 47.853
------train-----
1.7483981847763062 loss val
1.8906564712524414 loss val
1.6475001573562622 loss val
1.6716591119766235 loss val
1.6664576530456543 loss val
1.4386506080627441 loss val
1.7150019407272339 loss val
1.221906304359436 loss val
1.2010433673858643 loss val
1.38370943069458 loss val
1.248693823814392 loss val
1.2143316268920898 loss val
0.9842000007629395 loss val
1.0978758335113525 loss val
1.1384644508361816 loss val
1.1107399463653564 loss val
1.0673911571502686 loss val
1.2813067436218262 loss val
1.147060751914978 loss val
1.0751303434371948 loss val
1.0449577569961548 loss val
0.9285224676132202 loss val
1.0247076749801636 loss val
0.9471568465232849 loss val
1.0327273607254028 loss val
0.9136946797370911 loss val
----validating-----
 * Prec@1 29.448 Prec@3 55.624
------train-----
0.8036010265350342 loss val
0.8156144022941589 loss val
0.9347261190414429 loss val
0.8140832781791687 loss val
0.9352807402610779 loss val
0.954662024974823 loss val
0.756202220916748 loss val
0.7693238854408264 loss val
0.6537058353424072 loss val
0.7209838628768921 loss val
0.9137311577796936 loss val
0.6699069738388062 loss val
0.7267493605613708 loss val
0.831074059009552 loss val
0.7472553849220276 loss val
0.6349334120750427 loss val
0.8443333506584167 loss val
0.6809611320495605 loss val
0.6582469344139099 loss val
0.6893628835678101 loss val
0.6058973073959351 loss val
0.6571434736251831 loss val
0.8737266063690186 loss val
0.6520397663116455 loss val
0.8870662450790405 loss val
0.6963520050048828 loss val
----validating-----
 * Prec@1 35.787 Prec@3 58.691
------train-----
0.7126421332359314 loss val
0.6274528503417969 loss val
0.49337273836135864 loss val
0.6375548839569092 loss val
0.5984366536140442 loss val
0.4930011034011841 loss val
0.5249847173690796 loss val
0.48933178186416626 loss val
0.6326838135719299 loss val
0.44020941853523254 loss val
0.46675899624824524 loss val
0.33464550971984863 loss val
0.63017737865448 loss val
0.6899164319038391 loss val
0.44810962677001953 loss val
0.47405633330345154 loss val
0.6819158792495728 loss val
0.5610470771789551 loss val
0.42094069719314575 loss val
0.6059415936470032 loss val
0.6151793599128723 loss val
0.5582166314125061 loss val
0.5307679772377014 loss val
0.5016071200370789 loss val
0.5481553673744202 loss val
0.4143533408641815 loss val
----validating-----
 * Prec@1 37.832 Prec@3 61.145
------train-----
0.35570305585861206 loss val
0.28942033648490906 loss val
0.32150813937187195 loss val
0.3853132724761963 loss val
0.49430468678474426 loss val
0.36462295055389404 loss val
0.4411396086215973 loss val
0.4743754267692566 loss val
0.39386025071144104 loss val
0.3528534770011902 loss val
0.5398452281951904 loss val
0.4986703395843506 loss val
0.5720587372779846 loss val
0.5233162045478821 loss val
0.3593485653400421 loss val
0.33594441413879395 loss val
0.42548736929893494 loss val
0.4032019078731537 loss val
0.5220637321472168 loss val
0.4051192104816437 loss val
0.4312739074230194 loss val
0.48213157057762146 loss val
0.5143195390701294 loss val
0.37748292088508606 loss val
0.401824027299881 loss val
0.34258994460105896 loss val
----validating-----
 * Prec@1 35.378 Prec@3 59.918
------train-----
0.33621662855148315 loss val
0.3256746530532837 loss val
0.4378163516521454 loss val
0.4001210033893585 loss val
0.38841721415519714 loss val
0.3028622269630432 loss val
0.35216251015663147 loss val
0.3357296884059906 loss val
0.46645835041999817 loss val
0.3472653329372406 loss val
0.38015076518058777 loss val
0.3367987871170044 loss val
0.23500730097293854 loss val
0.3183484375476837 loss val
0.36740943789482117 loss val
0.38665735721588135 loss val
0.3052018880844116 loss val
0.3432580828666687 loss val
0.5798971652984619 loss val
0.3235495090484619 loss val
0.31536865234375 loss val
0.3757682740688324 loss val
0.30514711141586304 loss val
0.23460792005062103 loss val
0.376689076423645 loss val
0.4143022894859314 loss val
----validating-----
 * Prec@1 38.446 Prec@3 61.145
------train-----
0.2601246237754822 loss val
0.23297685384750366 loss val
0.26856356859207153 loss val
0.21944884955883026 loss val
0.2565424144268036 loss val
0.2745378613471985 loss val
0.2980445623397827 loss val
0.3042493760585785 loss val
0.28880053758621216 loss val
0.27683088183403015 loss val
0.3362456262111664 loss val
0.4115726351737976 loss val
0.29546070098876953 loss val
0.28624600172042847 loss val
0.19835293292999268 loss val
0.3000570237636566 loss val
0.5112325549125671 loss val
0.36167359352111816 loss val
0.3465278148651123 loss val
0.3141976296901703 loss val
0.30352652072906494 loss val
0.35611557960510254 loss val
0.20629502832889557 loss val
0.4053781032562256 loss val
0.2404707372188568 loss val
0.29839399456977844 loss val
----validating-----
 * Prec@1 36.401 Prec@3 61.145
------train-----
0.3106469213962555 loss val
0.3009600341320038 loss val
0.3415636718273163 loss val
0.21225737035274506 loss val
0.184636652469635 loss val
0.3322657644748688 loss val
0.261608749628067 loss val
0.16418956220149994 loss val
0.17285872995853424 loss val
0.2768445312976837 loss val
0.2544999420642853 loss val
0.20590095221996307 loss val
0.3574315011501312 loss val
0.26862281560897827 loss val
0.260357141494751 loss val
0.18350613117218018 loss val
0.1930904984474182 loss val
0.2264471799135208 loss val
0.24792881309986115 loss val
0.34852513670921326 loss val
0.17647704482078552 loss val
0.1907559633255005 loss val
0.3497288227081299 loss val
0.326233446598053 loss val
0.2618894577026367 loss val
0.254290372133255 loss val
----validating-----
 * Prec@1 38.855 Prec@3 62.168
------train-----
0.1745854616165161 loss val
0.31797245144844055 loss val
0.19540461897850037 loss val
0.29590508341789246 loss val
0.27285927534103394 loss val
0.3023684322834015 loss val
0.23419804871082306 loss val
0.23024573922157288 loss val
0.19609932601451874 loss val
0.23188555240631104 loss val
0.26137903332710266 loss val
0.24018457531929016 loss val
0.2392784059047699 loss val
0.22504645586013794 loss val
0.23483501374721527 loss val
0.21348728239536285 loss val
0.21608661115169525 loss val
0.2162620574235916 loss val
0.2148735374212265 loss val
0.31712624430656433 loss val
0.23077282309532166 loss val
0.22857439517974854 loss val
0.27976351976394653 loss val
0.18462349474430084 loss val
0.20991726219654083 loss val
0.19140177965164185 loss val
----validating-----
 * Prec@1 38.855 Prec@3 63.395
------train-----
0.22286787629127502 loss val
0.1920340657234192 loss val
0.20002523064613342 loss val
0.1744776964187622 loss val
0.208577498793602 loss val
0.23947519063949585 loss val
0.2304438203573227 loss val
0.21181714534759521 loss val
0.14722579717636108 loss val
0.19232332706451416 loss val
0.2576192617416382 loss val
0.29246723651885986 loss val
0.195083349943161 loss val
0.2503730058670044 loss val
0.19694271683692932 loss val
0.19698362052440643 loss val
0.2105536311864853 loss val
0.12172415107488632 loss val
0.17240968346595764 loss val
0.29491254687309265 loss val
0.31542617082595825 loss val
0.28480663895606995 loss val
0.20355571806430817 loss val
0.23375393450260162 loss val
0.1957707554101944 loss val
0.1339743733406067 loss val
----validating-----
 * Prec@1 36.605 Prec@3 62.986
------train-----
0.20776911079883575 loss val
0.17405755817890167 loss val
0.22007854282855988 loss val
0.14135713875293732 loss val
0.1571403592824936 loss val
0.28098833560943604 loss val
0.20552417635917664 loss val
0.26456132531166077 loss val
0.11240052431821823 loss val
0.24501362442970276 loss val
0.12542659044265747 loss val
0.3626002371311188 loss val
0.15692131221294403 loss val
0.1788969784975052 loss val
0.1454835683107376 loss val
0.21811150014400482 loss val
0.1797645390033722 loss val
0.19923526048660278 loss val
0.17858511209487915 loss val
0.36477553844451904 loss val
0.14350678026676178 loss val
0.18651308119297028 loss val
0.14380568265914917 loss val
0.17923575639724731 loss val
0.25241270661354065 loss val
0.12557558715343475 loss val
----validating-----
 * Prec@1 37.832 Prec@3 61.759
------train-----
0.1701183319091797 loss val
0.14025528728961945 loss val
0.17086899280548096 loss val
0.1165141612291336 loss val
0.14691925048828125 loss val
0.17507785558700562 loss val

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
